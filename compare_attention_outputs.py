#!/usr/bin/env python3
"""
å¯¹æ¯” H100 å’Œ B100 çš„ attention output tensor ç›¸ä¼¼åº¦
æ”¯æŒ .safetensors å’Œ .bin æ–‡ä»¶æ ¼å¼
"""

import argparse
import os
import sys

import numpy as np
import torch
from safetensors.torch import load_file


def load_bin_file(bin_path, shape, dtype='bfloat16', device='cpu'):
    """
    åŠ è½½äºŒè¿›åˆ¶æ–‡ä»¶ä¸º PyTorch tensor

    Args:
        bin_path: äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
        shape: tensor çš„ shapeï¼Œå¦‚ (25, 4096)
        dtype: æ•°æ®ç±»å‹ï¼Œå¦‚ 'bfloat16', 'float16', 'float32'
        device: è®¾å¤‡ï¼Œ'cpu' æˆ– 'cuda'

    Returns:
        PyTorch tensor
    """
    dtype_map = {
        'bfloat16': torch.bfloat16,
        'float16': torch.float16,
        'float32': torch.float32,
        'fp16': torch.float16,
        'fp32': torch.float32,
        'bf16': torch.bfloat16,
    }

    dtype_np_map = {
        'bfloat16': np.uint16,  # bfloat16 éœ€è¦ç‰¹æ®Šå¤„ç†
        'float16': np.float16,
        'float32': np.float32,
        'fp16': np.float16,
        'fp32': np.float32,
        'bf16': np.uint16,
    }

    dtype_map.get(dtype, torch.bfloat16)
    np_dtype = dtype_np_map.get(dtype, np.uint16)

    # è¯»å–äºŒè¿›åˆ¶æ–‡ä»¶
    with open(bin_path, 'rb') as f:
        data = np.fromfile(f, dtype=np_dtype)

    # æ£€æŸ¥æ•°æ®å¤§å°æ˜¯å¦åŒ¹é…
    expected_size = np.prod(shape)
    if len(data) != expected_size:
        print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤§å°ä¸åŒ¹é…! æœŸæœ› {expected_size} ä¸ªå…ƒç´ ï¼Œå®é™… {len(data)} ä¸ªå…ƒç´ ")
        print(f"   å°è¯•è‡ªåŠ¨æ¨æ–­ shape...")
        # å°è¯•æ¨æ–­ shape
        if len(data) % 4096 == 0:
            inferred_shape = (len(data) // 4096, 4096)
            print(f"   æ¨æ–­ shape ä¸º: {inferred_shape}")
            shape = inferred_shape
        else:
            print(f"   æ— æ³•æ¨æ–­åˆç†çš„ shapeï¼Œä½¿ç”¨åŸå§‹ shape å¹¶æˆªæ–­/å¡«å……")
            if len(data) < expected_size:
                # å¡«å…… 0
                data = np.pad(data, (0, expected_size - len(data)),
                              constant_values=0)
            else:
                # æˆªæ–­
                data = data[:expected_size]

    # Reshape
    data = data.reshape(shape)

    # è½¬æ¢ä¸º PyTorch tensor
    if dtype in ['bfloat16', 'bf16']:
        # bfloat16 éœ€è¦ç‰¹æ®Šå¤„ç†ï¼šå°† uint16 è§†å›¾è½¬æ¢ä¸º bfloat16
        tensor = torch.from_numpy(data).view(torch.bfloat16)
    else:
        tensor = torch.from_numpy(data)

    return tensor.to(device)


def load_safetensors_file(st_path):
    """
    åŠ è½½ safetensors æ–‡ä»¶

    Args:
        st_path: safetensors æ–‡ä»¶è·¯å¾„

    Returns:
        PyTorch tensor (ç¬¬ä¸€ä¸ª tensor)
    """
    data = load_file(st_path)
    if len(data) > 1:
        print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶åŒ…å«å¤šä¸ª tensor: {list(data.keys())}ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ª")
    return list(data.values())[0]


def load_tensor_file(file_path, shape=None, dtype='bfloat16', device='cpu'):
    """
    é€šç”¨æ–‡ä»¶åŠ è½½å‡½æ•°ï¼Œæ ¹æ®æ‰©å±•åè‡ªåŠ¨é€‰æ‹©åŠ è½½æ–¹å¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        shape: ä»…å¯¹ .bin æ–‡ä»¶éœ€è¦ï¼Œtensor çš„ shape
        dtype: ä»…å¯¹ .bin æ–‡ä»¶éœ€è¦ï¼Œæ•°æ®ç±»å‹
        device: è®¾å¤‡

    Returns:
        PyTorch tensor
    """
    ext = os.path.splitext(file_path)[1].lower()

    if ext == '.bin':
        if shape is None:
            raise ValueError(f"åŠ è½½ .bin æ–‡ä»¶éœ€è¦æŒ‡å®š shape å‚æ•°")
        print(f"âœ“ åŠ è½½ .bin æ–‡ä»¶: {file_path} (shape={shape}, dtype={dtype})")
        return load_bin_file(file_path, shape, dtype, device)
    elif ext == '.safetensors':
        print(f"âœ“ åŠ è½½ .safetensors æ–‡ä»¶: {file_path}")
        tensor = load_safetensors_file(file_path)
        return tensor.to(device)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}ï¼Œä»…æ”¯æŒ .bin å’Œ .safetensors")


def print_tensor_stats(tensor, name):
    """æ‰“å° tensor çš„ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n{name} ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  Shape: {tensor.shape}")
    print(f"  Dtype: {tensor.dtype}")
    print(f"  Min: {tensor.min().item():.6f}")
    print(f"  Max: {tensor.max().item():.6f}")
    print(f"  Mean: {tensor.mean().item():.6f}")
    print(f"  Std: {tensor.std().item():.6f}")
    print(f"  NaN count: {torch.isnan(tensor).sum().item()}")
    print(f"  Inf count: {torch.isinf(tensor).sum().item()}")


def compute_similarity_metrics(actual, expected, name="Comparison"):
    """è®¡ç®—è¯¦ç»†çš„ç›¸ä¼¼åº¦æŒ‡æ ‡"""
    print(f"\n{'='*80}")
    print(f"{name}")
    print(f"{'='*80}")

    # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
    if actual.device != expected.device:
        actual = actual.to(expected.device)

    # è½¬æ¢ä¸º float32 è¿›è¡Œç²¾ç¡®è®¡ç®—
    actual_f32 = actual.float()
    expected_f32 = expected.float()

    # 1. ç»å¯¹å·®å¼‚
    abs_diff = (actual_f32 - expected_f32).abs()
    max_abs_diff = abs_diff.max()
    max_abs_idx = abs_diff.argmax()
    max_abs_idx_2d = np.unravel_index(max_abs_idx.cpu().item(), actual.shape)

    print(f"\nğŸ“Š ç»å¯¹å·®å¼‚:")
    print(
        f"  Greatest absolute difference: {max_abs_diff:.6f} at index {max_abs_idx_2d}"
    )
    print(f"  Mean absolute difference: {abs_diff.mean():.6f}")
    print(f"  Median absolute difference: {abs_diff.median():.6f}")
    print(f"  Std of absolute difference: {abs_diff.std():.6f}")

    # 2. ç›¸å¯¹å·®å¼‚ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    denominator = torch.maximum(expected_f32.abs(), actual_f32.abs())
    significant_mask = denominator > 1e-3

    if significant_mask.any():
        rel_diff_significant = abs_diff[significant_mask] / (
            denominator[significant_mask] + 1e-8)
        max_rel_diff_sig = rel_diff_significant.max()
        max_rel_idx_flat = torch.where(
            significant_mask.flatten())[0][rel_diff_significant.argmax()]
        max_rel_idx_2d = np.unravel_index(max_rel_idx_flat.cpu().item(),
                                          actual.shape)

        print(f"\nğŸ“ˆ ç›¸å¯¹å·®å¼‚ (ä»…å¯¹æ˜¾è‘—å€¼ |value| > 0.001):")
        print(
            f"  æ˜¾è‘—å…ƒç´ æ•°é‡: {significant_mask.sum().item()} / {actual.numel()} ({100*significant_mask.sum().item()/actual.numel():.1f}%)"
        )
        print(
            f"  Greatest relative difference: {max_rel_diff_sig:.6f} ({max_rel_diff_sig*100:.2f}%) at index {max_rel_idx_2d}"
        )
        print(
            f"  Mean relative difference: {rel_diff_significant.mean():.6f} ({rel_diff_significant.mean()*100:.2f}%)"
        )
        print(
            f"  Median relative difference: {rel_diff_significant.median():.6f} ({rel_diff_significant.median()*100:.2f}%)"
        )
    else:
        print(f"\nğŸ“ˆ ç›¸å¯¹å·®å¼‚:")
        print(f"  æ‰€æœ‰å€¼éƒ½å¤ªå° (< 0.001)ï¼Œç›¸å¯¹å·®å¼‚ä¸é€‚ç”¨")

    # 3. ä¼ ç»Ÿç›¸å¯¹å·®å¼‚ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    rel_diff_traditional = abs_diff / (expected_f32.abs() + 1e-8)
    max_rel_diff_trad = rel_diff_traditional.max()
    max_rel_idx_trad = rel_diff_traditional.argmax()
    max_rel_idx_trad_2d = np.unravel_index(max_rel_idx_trad.cpu().item(),
                                           actual.shape)

    print(f"\nâš ï¸  ä¼ ç»Ÿç›¸å¯¹å·®å¼‚ (å¯èƒ½è¢«å°å€¼å¤¸å¤§):")
    print(f"  Greatest: {max_rel_diff_trad:.6f} at index {max_rel_idx_trad_2d}")
    print(
        f"    â†’ æœŸæœ›å€¼: {expected[max_rel_idx_trad_2d]:.6e}, å®é™…å€¼: {actual[max_rel_idx_trad_2d]:.6e}"
    )

    # 4. ä½™å¼¦ç›¸ä¼¼åº¦
    actual_flat = actual_f32.flatten()
    expected_flat = expected_f32.flatten()
    cos_sim = torch.nn.functional.cosine_similarity(actual_flat.unsqueeze(0),
                                                    expected_flat.unsqueeze(0))
    print(f"\nğŸ¯ ä½™å¼¦ç›¸ä¼¼åº¦: {cos_sim.item():.8f}")

    # 5. ç›¸å…³ç³»æ•°
    actual_centered = actual_flat - actual_flat.mean()
    expected_centered = expected_flat - expected_flat.mean()
    correlation = (actual_centered * expected_centered).sum() / (
        actual_centered.norm() * expected_centered.norm() + 1e-8)
    print(f"ğŸ“ Pearson ç›¸å…³ç³»æ•°: {correlation.item():.8f}")

    # 6. æœ€å¤§ç»å¯¹å·®å¼‚ä½ç½®çš„è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ” æœ€å¤§ç»å¯¹å·®å¼‚ä½ç½® (index {max_abs_idx_2d}):")
    print(f"  H100 å€¼: {expected[max_abs_idx_2d]:.6f}")
    print(f"  B100 å€¼: {actual[max_abs_idx_2d]:.6f}")
    print(f"  ç»å¯¹å·®å¼‚: {abs_diff[max_abs_idx_2d]:.6f}")
    if denominator[max_abs_idx_2d] > 1e-3:
        rel_at_max_abs = abs_diff[max_abs_idx_2d] / (
            denominator[max_abs_idx_2d] + 1e-8)
        print(f"  ç›¸å¯¹å·®å¼‚: {rel_at_max_abs:.6f} ({rel_at_max_abs*100:.2f}%)")

    if significant_mask.any():
        print(f"\nğŸ” æœ€å¤§ç›¸å¯¹å·®å¼‚ä½ç½® (index {max_rel_idx_2d}, ä»…æ˜¾è‘—å€¼):")
        print(f"  H100 å€¼: {expected[max_rel_idx_2d]:.6f}")
        print(f"  B100 å€¼: {actual[max_rel_idx_2d]:.6f}")
        print(f"  ç»å¯¹å·®å¼‚: {abs_diff[max_rel_idx_2d]:.6f}")
        print(f"  ç›¸å¯¹å·®å¼‚: {max_rel_diff_sig:.6f} ({max_rel_diff_sig*100:.2f}%)")

    # 7. å·®å¼‚æœ€å¤§çš„å‰10ä¸ªä½ç½®
    print(f"\nğŸ“‹ ç»å¯¹å·®å¼‚æœ€å¤§çš„å‰10ä¸ªä½ç½®:")
    flat_abs_diff = abs_diff.flatten()
    top_indices = flat_abs_diff.topk(10).indices
    for i, idx in enumerate(top_indices):
        idx_2d = np.unravel_index(idx.cpu().item(), actual.shape)
        denom = torch.maximum(expected[idx_2d].abs(), actual[idx_2d].abs())
        if denom > 1e-3:
            rel_pct = (abs_diff[idx_2d] / (denom + 1e-8) * 100).item()
            print(
                f"  {i+1:2d}. Index {idx_2d}: H100={expected[idx_2d]:.6f}, B100={actual[idx_2d]:.6f}, "
                f"diff={abs_diff[idx_2d]:.6f} ({rel_pct:.1f}%)")
        else:
            print(
                f"  {i+1:2d}. Index {idx_2d}: H100={expected[idx_2d]:.6f}, B100={actual[idx_2d]:.6f}, "
                f"diff={abs_diff[idx_2d]:.6f} (å€¼å¤ªå°)")

    # 8. è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
    print(f"\nğŸ“Š è¯¯å·®åˆ†å¸ƒ:")
    total = actual.numel()
    within_001 = (abs_diff <= 0.001).sum().item()
    within_01 = (abs_diff <= 0.01).sum().item()
    within_02 = (abs_diff <= 0.02).sum().item()
    within_05 = (abs_diff <= 0.05).sum().item()
    within_1 = (abs_diff <= 0.1).sum().item()

    print(
        f"  |diff| â‰¤ 0.001: {within_001:6d} / {total} ({100*within_001/total:.1f}%)"
    )
    print(
        f"  |diff| â‰¤ 0.01 : {within_01:6d} / {total} ({100*within_01/total:.1f}%)"
    )
    print(
        f"  |diff| â‰¤ 0.02 : {within_02:6d} / {total} ({100*within_02/total:.1f}%)"
    )
    print(
        f"  |diff| â‰¤ 0.05 : {within_05:6d} / {total} ({100*within_05/total:.1f}%)"
    )
    print(
        f"  |diff| â‰¤ 0.1  : {within_1:6d} / {total} ({100*within_1/total:.1f}%)"
    )

    # 9. ç›¸å¯¹è¯¯å·®åˆ†å¸ƒï¼ˆå¯¹æ˜¾è‘—å€¼ï¼‰
    if significant_mask.any():
        print(f"\nğŸ“Š ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ (ä»…æ˜¾è‘—å€¼):")
        sig_count = significant_mask.sum().item()
        rel_diff_sig = abs_diff[significant_mask] / (
            denominator[significant_mask] + 1e-8)
        within_1pct = (rel_diff_sig <= 0.01).sum().item()
        within_5pct = (rel_diff_sig <= 0.05).sum().item()
        within_10pct = (rel_diff_sig <= 0.10).sum().item()

        print(
            f"  ç›¸å¯¹è¯¯å·® â‰¤ 1% : {within_1pct:6d} / {sig_count} ({100*within_1pct/sig_count:.1f}%)"
        )
        print(
            f"  ç›¸å¯¹è¯¯å·® â‰¤ 5% : {within_5pct:6d} / {sig_count} ({100*within_5pct/sig_count:.1f}%)"
        )
        print(
            f"  ç›¸å¯¹è¯¯å·® â‰¤ 10%: {within_10pct:6d} / {sig_count} ({100*within_10pct/sig_count:.1f}%)"
        )

    # 10. PyTorch assert_close æµ‹è¯•
    print(f"\nâœ… PyTorch assert_close æµ‹è¯•:")
    for atol, rtol in [(0.001, 0.001), (0.01, 0.01), (0.02, 0.02),
                       (0.05, 0.05)]:
        try:
            torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)
            print(f"  âœ“ PASS with atol={atol}, rtol={rtol}")
        except AssertionError:
            print(f"  âœ— FAIL with atol={atol}, rtol={rtol}")

    print(f"\n{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(
        description='å¯¹æ¯”ä¸¤ä¸ª tensor æ–‡ä»¶çš„ç›¸ä¼¼åº¦ï¼ˆæ”¯æŒ .safetensors å’Œ .bin æ ¼å¼ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¹æ¯”ä¸¤ä¸ª safetensors æ–‡ä»¶
  %(prog)s h100_attention0_output.safetensors b100_attention0_output.safetensors

  # å¯¹æ¯” bin æ–‡ä»¶å’Œ safetensors æ–‡ä»¶
  %(prog)s fmha_output_trtllm_gen.bin b100_attention0_output.safetensors --shape 25 4096

  # æŒ‡å®š dtype
  %(prog)s file1.bin file2.bin --shape 25 4096 --dtype float16
        """)

    parser.add_argument('file1', help='ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆä½œä¸ºæœŸæœ›å€¼/å‚è€ƒå€¼ï¼‰')
    parser.add_argument('file2', help='ç¬¬äºŒä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆä½œä¸ºå®é™…å€¼/å¯¹æ¯”å€¼ï¼‰')
    parser.add_argument('--shape',
                        nargs='+',
                        type=int,
                        help='å¯¹äº .bin æ–‡ä»¶ï¼ŒæŒ‡å®š tensor shapeï¼Œå¦‚: --shape 25 4096')
    parser.add_argument(
        '--dtype',
        default='bfloat16',
        choices=['bfloat16', 'bf16', 'float16', 'fp16', 'float32', 'fp32'],
        help='å¯¹äº .bin æ–‡ä»¶ï¼ŒæŒ‡å®šæ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)')
    parser.add_argument('--device',
                        default='cpu',
                        choices=['cpu', 'cuda'],
                        help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: cpu)')

    args = parser.parse_args()

    # å¤„ç† shape å‚æ•°
    shape = None
    if args.shape:
        shape = tuple(args.shape)

    print("ğŸ” åŠ è½½æ–‡ä»¶...")
    print(f"  æ–‡ä»¶1 (æœŸæœ›/å‚è€ƒ): {args.file1}")
    print(f"  æ–‡ä»¶2 (å®é™…/å¯¹æ¯”): {args.file2}")

    try:
        # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶
        tensor1 = load_tensor_file(args.file1,
                                   shape=shape,
                                   dtype=args.dtype,
                                   device=args.device)

        # åŠ è½½ç¬¬äºŒä¸ªæ–‡ä»¶
        tensor2 = load_tensor_file(args.file2,
                                   shape=shape,
                                   dtype=args.dtype,
                                   device=args.device)

    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # æ‰“å°åŸºæœ¬ä¿¡æ¯
    print_tensor_stats(tensor1, f"æ–‡ä»¶1: {os.path.basename(args.file1)}")
    print_tensor_stats(tensor2, f"æ–‡ä»¶2: {os.path.basename(args.file2)}")

    # æ£€æŸ¥ shape æ˜¯å¦åŒ¹é…
    if tensor1.shape != tensor2.shape:
        print(f"\nâŒ é”™è¯¯: Tensor shape ä¸åŒ¹é…!")
        print(f"  æ–‡ä»¶1 shape: {tensor1.shape}")
        print(f"  æ–‡ä»¶2 shape: {tensor2.shape}")
        sys.exit(1)

    print(f"\nâœ“ Shape åŒ¹é…: {tensor1.shape}")

    # è®¡ç®—ç›¸ä¼¼åº¦
    comparison_name = f"{os.path.basename(args.file2)} vs {os.path.basename(args.file1)}"
    compute_similarity_metrics(tensor2, tensor1, comparison_name)

    # å¦‚æœæ˜¯ 2D tensorï¼Œè¿˜å¯ä»¥æŒ‰è¡Œåˆ†æ
    if len(tensor1.shape) == 2:
        print(f"\n{'='*80}")
        print("ğŸ“Š æŒ‰è¡Œåˆ†æï¼ˆå‰5è¡Œå’Œå5è¡Œï¼‰")
        print(f"{'='*80}")

        num_rows = tensor1.shape[0]
        rows_to_check = list(range(min(5, num_rows))) + list(
            range(max(num_rows - 5, 5), num_rows))

        for row_idx in rows_to_check:
            row1 = tensor1[row_idx]
            row2 = tensor2[row_idx]

            abs_diff = (row1.float() - row2.float()).abs()
            max_diff = abs_diff.max()
            mean_diff = abs_diff.mean()

            print(
                f"  Row {row_idx:2d}: max_diff={max_diff:.6f}, mean_diff={mean_diff:.6f}"
            )


if __name__ == "__main__":
    main()
