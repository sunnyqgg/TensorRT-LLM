#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°† 61 ä¸ªå•ç‹¬çš„ key token æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„ KV cache key æ–‡ä»¶
"""

import argparse
import os

import numpy as np


def load_single_key_token(filepath, num_kv_heads, head_dim, dtype=np.float16):
    """
    åŠ è½½å•ä¸ª token çš„ key æ•°æ®

    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        num_kv_heads: KV head æ•°é‡
        head_dim: æ¯ä¸ª head çš„ç»´åº¦
        dtype: æ•°æ®ç±»å‹

    Returns:
        shape [num_kv_heads, head_dim] çš„ numpy æ•°ç»„
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

    data = np.fromfile(filepath, dtype=dtype)
    expected_elements = num_kv_heads * head_dim

    if data.size != expected_elements:
        raise ValueError(f"æ–‡ä»¶ {filepath} çš„å…ƒç´ æ•°é‡ä¸åŒ¹é…: "
                         f"æœŸæœ› {expected_elements} ({num_kv_heads}Ã—{head_dim}), "
                         f"å®é™… {data.size}")

    return data.reshape(num_kv_heads, head_dim)


def concat_all_keys(dump_dir,
                    total_tokens,
                    num_kv_heads,
                    head_dim,
                    dtype=np.float16):
    """
    åˆå¹¶æ‰€æœ‰ token çš„ key æ•°æ®

    Args:
        dump_dir: dump æ–‡ä»¶æ‰€åœ¨ç›®å½•
        total_tokens: token æ€»æ•°ï¼ˆé»˜è®¤ 61ï¼‰
        num_kv_heads: KV head æ•°é‡
        head_dim: æ¯ä¸ª head çš„ç»´åº¦
        dtype: æ•°æ®ç±»å‹

    Returns:
        shape [total_tokens, num_kv_heads, head_dim] çš„ numpy æ•°ç»„
    """
    all_keys = []

    print(f"æ­£åœ¨åŠ è½½ {total_tokens} ä¸ª token çš„ key æ•°æ®...")
    print(
        f"é…ç½®: num_kv_heads={num_kv_heads}, head_dim={head_dim}, dtype={dtype}")
    print("-" * 60)

    for token_idx in range(total_tokens):
        filename = os.path.join(dump_dir, f"key_token_idx_[{token_idx}].bin")

        try:
            key_data = load_single_key_token(filename, num_kv_heads, head_dim,
                                             dtype)
            all_keys.append(key_data)

            if token_idx % 10 == 0 or token_idx == total_tokens - 1:
                print(f"  å·²åŠ è½½ Token {token_idx:3d}: shape={key_data.shape}, "
                      f"min={key_data.min():.4f}, max={key_data.max():.4f}, "
                      f"mean={key_data.mean():.4f}")

        except Exception as e:
            print(f"  âš ï¸  åŠ è½½ Token {token_idx} å¤±è´¥: {e}")
            raise

    # æ²¿ç€ token ç»´åº¦ concat
    concatenated_keys = np.stack(all_keys, axis=0)

    print("-" * 60)
    print(f"âœ… æˆåŠŸåˆå¹¶æ‰€æœ‰ keys!")
    print(f"   æœ€ç»ˆ shape: {concatenated_keys.shape}")
    print(f"   æ€»å¤§å°: {concatenated_keys.nbytes / 1024:.2f} KB "
          f"({concatenated_keys.nbytes / (1024*1024):.2f} MB)")
    print(
        f"   æ•°æ®èŒƒå›´: min={concatenated_keys.min():.4f}, "
        f"max={concatenated_keys.max():.4f}, mean={concatenated_keys.mean():.4f}"
    )

    return concatenated_keys


def concat_all_values(dump_dir,
                      total_tokens,
                      num_kv_heads,
                      head_dim,
                      dtype=np.float16):
    """
    åˆå¹¶æ‰€æœ‰ token çš„ value æ•°æ®

    Args:
        dump_dir: dump æ–‡ä»¶æ‰€åœ¨ç›®å½•
        total_tokens: token æ€»æ•°ï¼ˆé»˜è®¤ 61ï¼‰
        num_kv_heads: KV head æ•°é‡
        head_dim: æ¯ä¸ª head çš„ç»´åº¦
        dtype: æ•°æ®ç±»å‹

    Returns:
        shape [total_tokens, num_kv_heads, head_dim] çš„ numpy æ•°ç»„
    """
    all_values = []

    print(f"æ­£åœ¨åŠ è½½ {total_tokens} ä¸ª token çš„ value æ•°æ®...")
    print(
        f"é…ç½®: num_kv_heads={num_kv_heads}, head_dim={head_dim}, dtype={dtype}")
    print("-" * 60)

    for token_idx in range(total_tokens):
        filename = os.path.join(dump_dir, f"v_key_token_idx_[{token_idx}].bin")

        try:
            value_data = load_single_key_token(filename, num_kv_heads, head_dim,
                                               dtype)
            all_values.append(value_data)

            if token_idx % 10 == 0 or token_idx == total_tokens - 1:
                print(
                    f"  å·²åŠ è½½ Token {token_idx:3d}: shape={value_data.shape}, "
                    f"min={value_data.min():.4f}, max={value_data.max():.4f}, "
                    f"mean={value_data.mean():.4f}")

        except Exception as e:
            print(f"  âš ï¸  åŠ è½½ Token {token_idx} å¤±è´¥: {e}")
            raise

    # æ²¿ç€ token ç»´åº¦ concat
    concatenated_values = np.stack(all_values, axis=0)

    print("-" * 60)
    print(f"âœ… æˆåŠŸåˆå¹¶æ‰€æœ‰ values!")
    print(f"   æœ€ç»ˆ shape: {concatenated_values.shape}")
    print(f"   æ€»å¤§å°: {concatenated_values.nbytes / 1024:.2f} KB "
          f"({concatenated_values.nbytes / (1024*1024):.2f} MB)")
    print(
        f"   æ•°æ®èŒƒå›´: min={concatenated_values.min():.4f}, "
        f"max={concatenated_values.max():.4f}, mean={concatenated_values.mean():.4f}"
    )

    return concatenated_values


def save_concatenated_keys(keys, output_path):
    """
    ä¿å­˜åˆå¹¶åçš„ keys

    Args:
        keys: åˆå¹¶åçš„ numpy æ•°ç»„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ä¿å­˜ä¸º binary æ–‡ä»¶
    keys.tofile(output_path)
    print(f"\nğŸ’¾ å·²ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶: {output_path}")

    # åŒæ—¶ä¿å­˜ä¸º .npy æ ¼å¼ï¼ˆæ–¹ä¾¿åç»­åŠ è½½ï¼‰
    npy_path = output_path.replace('.bin', '.npy')
    np.save(npy_path, keys)
    print(f"ğŸ’¾ å·²ä¿å­˜ä¸º .npy æ ¼å¼: {npy_path}")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = output_path.replace('.bin', '_stats.txt')
    with open(stats_path, 'w') as f:
        f.write(f"Shape: {keys.shape}\n")
        f.write(f"Dtype: {keys.dtype}\n")
        f.write(f"Total elements: {keys.size}\n")
        f.write(f"Size (bytes): {keys.nbytes}\n")
        f.write(f"Min: {keys.min()}\n")
        f.write(f"Max: {keys.max()}\n")
        f.write(f"Mean: {keys.mean()}\n")
        f.write(f"Std: {keys.std()}\n")
        f.write(f"\nPer-token statistics:\n")
        for i in range(min(10, keys.shape[0])):
            f.write(f"  Token {i}: min={keys[i].min():.4f}, "
                    f"max={keys[i].max():.4f}, mean={keys[i].mean():.4f}\n")
        if keys.shape[0] > 10:
            f.write(f"  ...\n")
            f.write(f"  Token {keys.shape[0]-1}: min={keys[-1].min():.4f}, "
                    f"max={keys[-1].max():.4f}, mean={keys[-1].mean():.4f}\n")

    print(f"ğŸ“Š å·²ä¿å­˜ç»Ÿè®¡ä¿¡æ¯: {stats_path}")


def verify_concatenated_keys(keys, dump_dir, num_samples=3, is_value=False):
    """
    éªŒè¯åˆå¹¶åçš„ keys/values æ˜¯å¦æ­£ç¡®

    Args:
        keys: åˆå¹¶åçš„ numpy æ•°ç»„ [total_tokens, num_kv_heads, head_dim]
        dump_dir: åŸå§‹æ–‡ä»¶ç›®å½•
        num_samples: éªŒè¯çš„æ ·æœ¬æ•°é‡
        is_value: æ˜¯å¦æ˜¯ value æ•°æ®
    """
    data_type = "values" if is_value else "keys"
    print(f"\nğŸ” éªŒè¯åˆå¹¶ç»“æœ (æŠ½æŸ¥ {num_samples} ä¸ª token {data_type})...")
    print("-" * 60)

    total_tokens = keys.shape[0]
    num_kv_heads = keys.shape[1]
    head_dim = keys.shape[2]

    # éšæœºé€‰æ‹©å‡ ä¸ª token è¿›è¡ŒéªŒè¯
    sample_indices = np.linspace(0, total_tokens - 1, num_samples, dtype=int)

    all_match = True
    for token_idx in sample_indices:
        if is_value:
            filename = os.path.join(dump_dir,
                                    f"v_key_token_idx_[{token_idx}].bin")
        else:
            filename = os.path.join(dump_dir,
                                    f"key_token_idx_[{token_idx}].bin")

        original = load_single_key_token(filename, num_kv_heads, head_dim,
                                         keys.dtype)
        concatenated = keys[token_idx]

        if np.allclose(original, concatenated, rtol=1e-5, atol=1e-8):
            print(f"  âœ… Token {token_idx}: å®Œå…¨åŒ¹é…")
        else:
            print(f"  âŒ Token {token_idx}: ä¸åŒ¹é…!")
            max_diff = np.abs(original - concatenated).max()
            print(f"     æœ€å¤§å·®å¼‚: {max_diff}")
            all_match = False

    print("-" * 60)
    if all_match:
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰æŠ½æŸ¥çš„ {data_type} token éƒ½åŒ¹é…!")
    else:
        print(f"âŒ éªŒè¯å¤±è´¥ï¼šå­˜åœ¨ä¸åŒ¹é…çš„ {data_type} token!")

    return all_match


def main():
    parser = argparse.ArgumentParser(
        description='å°†å¤šä¸ªå•ç‹¬çš„ key/value token æ–‡ä»¶åˆå¹¶æˆå®Œæ•´çš„ KV cache æ–‡ä»¶')
    parser.add_argument('--dump_dir',
                        type=str,
                        default='./dump_data',
                        help='dump æ–‡ä»¶æ‰€åœ¨ç›®å½• (é»˜è®¤: ./dump_data)')
    parser.add_argument('--total_tokens',
                        type=int,
                        default=61,
                        help='token æ€»æ•° (é»˜è®¤: 61)')
    parser.add_argument('--num_kv_heads',
                        type=int,
                        default=8,
                        help='KV head æ•°é‡ (é»˜è®¤: 8)')
    parser.add_argument('--head_dim',
                        type=int,
                        default=128,
                        help='æ¯ä¸ª head çš„ç»´åº¦ (é»˜è®¤: 128)')
    parser.add_argument(
        '--dtype',
        type=str,
        default='float16',
        choices=['float16', 'bfloat16', 'float32', 'int8', 'uint8'],
        help='æ•°æ®ç±»å‹ (é»˜è®¤: float16)')
    parser.add_argument(
        '--output',
        type=str,
        default='./dump_data/all_keys_concatenated.bin',
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./dump_data/all_keys_concatenated.bin)')
    parser.add_argument('--type',
                        type=str,
                        default='both',
                        choices=['keys', 'values', 'both'],
                        help='é€‰æ‹©åˆå¹¶ keysã€values è¿˜æ˜¯ä¸¤è€…éƒ½åˆå¹¶ (é»˜è®¤: both)')
    parser.add_argument('--no_verify', action='store_true', help='è·³è¿‡éªŒè¯æ­¥éª¤')

    args = parser.parse_args()

    # æ•°æ®ç±»å‹æ˜ å°„
    dtype_map = {
        'float16': np.float16,
        'bfloat16': np.uint16,  # bfloat16 éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œå…ˆç”¨ uint16
        'float32': np.float32,
        'int8': np.int8,
        'uint8': np.uint8,
    }

    dtype = dtype_map[args.dtype]

    print("=" * 60)
    print("KV Cache åˆå¹¶å·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.dump_dir):
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {args.dump_dir}")
        return

    try:
        # æ ¹æ® type å‚æ•°å†³å®šå¤„ç†å“ªäº›æ•°æ®
        if args.type in ['keys', 'both']:
            print("\n" + "=" * 60)
            print("å¤„ç† Keys")
            print("=" * 60)

            # åˆå¹¶æ‰€æœ‰ keys
            concatenated_keys = concat_all_keys(args.dump_dir,
                                                args.total_tokens,
                                                args.num_kv_heads,
                                                args.head_dim, dtype)

            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if args.type == 'both':
                keys_output = args.output.replace('.bin', '_keys.bin')
            else:
                keys_output = args.output

            # ä¿å­˜ç»“æœ
            save_concatenated_keys(concatenated_keys, keys_output)

            # éªŒè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not args.no_verify:
                verify_concatenated_keys(concatenated_keys,
                                         args.dump_dir,
                                         num_samples=5,
                                         is_value=False)

        if args.type in ['values', 'both']:
            print("\n" + "=" * 60)
            print("å¤„ç† Values")
            print("=" * 60)

            # åˆå¹¶æ‰€æœ‰ values
            concatenated_values = concat_all_values(args.dump_dir,
                                                    args.total_tokens,
                                                    args.num_kv_heads,
                                                    args.head_dim, dtype)

            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if args.type == 'both':
                values_output = args.output.replace('.bin', '_values.bin')
            else:
                values_output = args.output

            # ä¿å­˜ç»“æœ
            save_concatenated_keys(concatenated_values, values_output)

            # éªŒè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not args.no_verify:
                verify_concatenated_keys(concatenated_values,
                                         args.dump_dir,
                                         num_samples=5,
                                         is_value=True)

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ“ä½œå®Œæˆ!")
        print("=" * 60)

        print("\nğŸ“ åç»­ä½¿ç”¨æ–¹æ³•:")
        print("```python")
        print("import numpy as np")
        print()

        if args.type in ['keys', 'both']:
            output_npy = keys_output.replace('.bin', '.npy')
            print(f"# åŠ è½½ Keys")
            print(f"keys = np.load('{output_npy}')")
            print(
                f"print(keys.shape)  # åº”è¯¥æ˜¯ ({args.total_tokens}, {args.num_kv_heads}, {args.head_dim})"
            )
            print()

        if args.type in ['values', 'both']:
            output_npy = values_output.replace('.bin', '.npy')
            print(f"# åŠ è½½ Values")
            print(f"values = np.load('{output_npy}')")
            print(
                f"print(values.shape)  # åº”è¯¥æ˜¯ ({args.total_tokens}, {args.num_kv_heads}, {args.head_dim})"
            )
            print()

        if args.type == 'both':
            print("# è®¿é—®ç‰¹å®š token çš„æ•°æ®")
            print(
                "token_5_key = keys[5]      # shape: (num_kv_heads, head_dim)")
            print(
                "token_5_value = values[5]  # shape: (num_kv_heads, head_dim)")

        print("```")

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
